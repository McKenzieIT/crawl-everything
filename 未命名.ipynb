{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import threading\n",
    "import pymongo\n",
    "import random\n",
    "import logging\n",
    "import json\n",
    "import time\n",
    "from queue import Queue\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mongo():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._client = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "        self._db = None\n",
    "        self._collection = None\n",
    "    \n",
    "    def insert(self, doc):\n",
    "        if type(doc) is dict:\n",
    "            result = self._collection.insert_one(doc)\n",
    "        else:\n",
    "            result = self._collection.insert_many(doc)\n",
    "        return result\n",
    "\n",
    "    def get_collection(self):\n",
    "        return self._collection\n",
    "    \n",
    "    def find_one(self, doc=None):\n",
    "        if doc is not None:\n",
    "            result = self._collection.find_one(doc)\n",
    "        else:\n",
    "            result = self._collection.find_one()\n",
    "        return result\n",
    "    \n",
    "    def find(self, doc=None):\n",
    "        if doc is not None:\n",
    "            result = self._collection.find(doc)\n",
    "        else:\n",
    "            result = self._collection.find()\n",
    "        return result\n",
    "    \n",
    "    def count(self, doc=None):\n",
    "        if doc is not None:\n",
    "            count = self._collection.find(doc).count()\n",
    "        else:\n",
    "            count = self._collection.find().count()\n",
    "        return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MongoDouban(Mongo):\n",
    "    __instance_lock = threading.Lock()\n",
    "    __init_flag = False\n",
    "    \n",
    "    def __init__(self):\n",
    "        if self.__init_flag is False:\n",
    "            super(MongoDouban, self).__init__()\n",
    "            self._db = self._client[\"douban\"]\n",
    "            self.__init_flag = True\n",
    "    \n",
    "    def __new__(cls, *args, **kwargs):\n",
    "        if not hasattr(MongoDouban, \"_instance\"):\n",
    "            with MongoDouban.__instance_lock:\n",
    "                if not hasattr(MongoDouban, \"_instance\"): \n",
    "                    MongoDouban._instance = object.__new__(cls)\n",
    "        return MongoDouban._instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubanMovieCollection(MongoDouban):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(DoubanMovieCollection, self).__init__()\n",
    "        self._collection = self._db['movie']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MongoDouban():\n",
    "    __instance_lock = threading.Lock()\n",
    "    __init_flag = False\n",
    "    _db = None\n",
    "    _collection = None\n",
    "    \n",
    "    def __init__(self):\n",
    "        if self.__init_flag is False:\n",
    "            print('init')\n",
    "            mongo_client = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "            self._db = mongo_client[\"douban\"]\n",
    "            self.__init_flag = True\n",
    "    \n",
    "    def __new__(cls, *args, **kwargs):\n",
    "        if not hasattr(MongoDouban, \"_instance\"):\n",
    "            with MongoDouban.__instance_lock:\n",
    "                if not hasattr(MongoDouban, \"_instance\"): \n",
    "                    MongoDouban._instance = object.__new__(cls)\n",
    "        return MongoDouban._instance\n",
    "    \n",
    "    def insert(self, doc):\n",
    "        if type(doc) is dict:\n",
    "            result = self._collection.insert_one(doc)\n",
    "        else:\n",
    "            result = self._collection.insert_many(doc)\n",
    "        return result\n",
    "\n",
    "    def get_collection(self):\n",
    "        return self._collection\n",
    "    \n",
    "    def find_one(self, doc=None):\n",
    "        if doc is not None:\n",
    "            result = self._collection.find_one(doc)\n",
    "        else:\n",
    "            result = self._collection.find_one()\n",
    "        return result\n",
    "    \n",
    "    def find(self, doc=None):\n",
    "        if doc is not None:\n",
    "            result = self._collection.find(doc)\n",
    "        else:\n",
    "            result = self._collection.find()\n",
    "        return result\n",
    "    \n",
    "    def count(self, doc=None):\n",
    "        if doc is not None:\n",
    "            count = self._collection.find(doc).count()\n",
    "        else:\n",
    "            count = self._collection.find().count()\n",
    "        return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_time(func):\n",
    "    def wrapper(*args, **kw):\n",
    "        start = time.time()\n",
    "        func(*args, **kw)\n",
    "        end = time.time()\n",
    "        print('running', end-start, 's')\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserAgent():\n",
    "    __instance_lock = threading.Lock()\n",
    "    __init_flag = False\n",
    "    \n",
    "    def __init__(self):\n",
    "        if self.__init_flag is False:\n",
    "            print('init UserAgent')\n",
    "            self.__agents_pool = list()\n",
    "            with open('./UserAgents/useragents.txt','r') as read_ob:\n",
    "                for line in read_ob.readlines():\n",
    "                    self.__agents_pool.append(line.strip())\n",
    "            self.__init_flag = True\n",
    "    \n",
    "    def __new__(cls, *args, **kwargs):\n",
    "        if not hasattr(UserAgent, \"_instance\"):\n",
    "            print('new UserAgent')\n",
    "            with UserAgent.__instance_lock:\n",
    "                if not hasattr(UserAgent, \"_instance\"): \n",
    "                    UserAgent._instance = object.__new__(cls)\n",
    "        return UserAgent._instance\n",
    "    \n",
    "    def get_useragent_randomly(self):\n",
    "        return random.choice(self.__agents_pool)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Crawl():\n",
    "\n",
    "    def __init__(self):\n",
    "        self._session = None\n",
    "        self._headers = {\n",
    "            'User-Agent':UserAgent().get_useragent_randomly(),\n",
    "            'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "            'Accept-Language':'zh-cn',\n",
    "            'Connection': 'keep-alive',\n",
    "            'Accept-Encoding': 'gzip, deflate',\n",
    "        }\n",
    "        self._proxies = [\n",
    "            {\"http\":\"112.111.217.114:9999\"},\n",
    "            {\"http\":\"180.118.128.118:9000\"},\n",
    "            {\"http\":\"171.11.29.217:9999\"},\n",
    "            {\"http\":\"120.83.109.191:9999\"}\n",
    "        ]\n",
    "        \n",
    "    def request_get(self, url, **kwargs):\n",
    "        logging.info('scraping {}...'.format(url))\n",
    "        try:\n",
    "            response = requests.get(url, headers=self._headers, proxies=random.choice(self._proxies), **kwargs)\n",
    "            if response.status_code == 200:\n",
    "                return response\n",
    "            logging.error('get invalid status code %s while scraping %s', response.status_code, url)\n",
    "        except requests.RequestException:\n",
    "            logging.error('error occurred while scraping %s', url, exc_info=True)\n",
    "        else:\n",
    "            logging.info('scraping {} finished'.format(url))\n",
    "     \n",
    "    def request_post(self, url, **kwargs):\n",
    "        logging.info('scraping {}...'.format(url))\n",
    "        try:\n",
    "            response = requests.post(url, headers=self._headers, proxies=random.choice(self._proxies), **kwargs)\n",
    "            if response.status_code == 200:\n",
    "                return response\n",
    "            logging.error('get invalid status code %s while scraping %s', response.status_code, url)\n",
    "        except requests.RequestException:\n",
    "            logging.error('error occurred while scraping %s', url, exc_info=True)\n",
    "        else:\n",
    "            logging.info('scraping {} finished'.format(url))\n",
    "    \n",
    "    def session_get(self, url, **kwargs):\n",
    "        self.check_session()\n",
    "        logging.info('scraping {}...'.format(url))\n",
    "        try:\n",
    "            response = self._session.get(url, headers=self._headers, proxies=random.choice(self._proxies), **kwargs)\n",
    "            if response.status_code == 200:\n",
    "                return response\n",
    "            logging.error('get invalid status code %s while scraping %s', response.status_code, url)\n",
    "        except requests.RequestException:\n",
    "            logging.error('error occurred while scraping %s', url, exc_info=True)\n",
    "        else:\n",
    "            logging.info('scraping {} finished'.format(url))\n",
    "    \n",
    "    def session_post(self, url, **kwargs):\n",
    "        self.check_session()\n",
    "        logging.info('scraping {}...'.format(url))\n",
    "        try:\n",
    "            response = self._session.post(url, headers=self._headers, proxies=random.choice(self._proxies), **kwargs)\n",
    "            if response.status_code == 200:\n",
    "                return response\n",
    "            logging.error('get invalid status code %s while scraping %s', response.status_code, url)\n",
    "        except requests.RequestException:\n",
    "            logging.error('error occurred while scraping %s', url, exc_info=True)\n",
    "        else:\n",
    "            logging.info('scraping {} finished'.format(url)) \n",
    "\n",
    "    def get_session(self):\n",
    "        return self._session\n",
    "    \n",
    "    def set_session(self, session):\n",
    "        self._session = session\n",
    "        return True\n",
    "    \n",
    "    def save_cookies(self, user):\n",
    "        cookieJar = requests.cookies.RequestsCookieJar()\n",
    "        for cookie in user.cookies:\n",
    "            cookieJar.set(cookie.name,cookie.value)\n",
    "        for cookie in user.headers['Set-Cookie'].split(\";\"):\n",
    "            key = cookie.split('=')[0]\n",
    "            value = cookie.split('=')[1]\n",
    "            cookieJar.set(key,value)\n",
    "        return cookieJar\n",
    "    \n",
    "    def check_session(self):\n",
    "        if self._session == None:\n",
    "            self._session = requests.session()\n",
    "        return\n",
    "    \n",
    "    def add_header(self, headers):\n",
    "        for key, value in headers.items():\n",
    "            self._headers[key] = value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubanUser(Crawl):\n",
    "    __instance_lock = threading.Lock()\n",
    "    __init_flag = False\n",
    "       \n",
    "    def __init__(self):\n",
    "        if self.__init_flag is False:\n",
    "            super(DoubanUser, self).__init__()\n",
    "            self.__user = None\n",
    "    #         self._cookies = None\n",
    "            self.__login()\n",
    "            self.__init_flag = True\n",
    "\n",
    "    def __new__(cls, *args, **kwargs):\n",
    "        if not hasattr(DoubanUser, \"_instance\"):\n",
    "            print('new Douban')\n",
    "            with DoubanUser.__instance_lock:\n",
    "                if not hasattr(Douban, \"_instance\"): \n",
    "                    DoubanUser._instance = object.__new__(cls)\n",
    "        return DoubanUser._instance\n",
    "    \n",
    "    def __login(self):\n",
    "        post_data = {\n",
    "            'name':'18664678368',\n",
    "            'password':'Ljc970412',\n",
    "            'remember':'false'\n",
    "        }\n",
    "        self.add_header({\n",
    "            \"Referer\":'https://accounts.douban.com/passport/login'\n",
    "        })\n",
    "        user = self.session_post('https://accounts.douban.com/j/mobile/login/basic', data=post_data)\n",
    "        login_detail = json.loads(user.text)\n",
    "        if login_detail['status'] == 'success':\n",
    "            print('login success!')\n",
    "#             self.__cookies = self.save_cookies(user)\n",
    "        else:\n",
    "            print('login failed!')\n",
    "        self.__user =  user.text\n",
    "    \n",
    "    def get_user_info(self):\n",
    "        return self.__user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Douban(Crawl):\n",
    "    \n",
    "    def __init__(self, douban_user):\n",
    "        super(Douban, self).__init__()\n",
    "        self.set_session(douban_user.get_session())\n",
    "    \n",
    "    def search(self, query, cat=''):\n",
    "        res = dict()\n",
    "        params = {\n",
    "            'q':query,\n",
    "            'cat':cat\n",
    "        }\n",
    "        count=0\n",
    "        response = self.session_get('https://www.douban.com/search', params=params)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        results = soup.find(class_='result-list').find_all(class_='result')\n",
    "        for result in results:\n",
    "            pic = result.find(class_='pic')\n",
    "            content = result.find(class_='content')\n",
    "            img = pic.img.get('src')\n",
    "            link = content.a.get('href')\n",
    "            name = content.a.text\n",
    "            description = content.p.text if content.p else ''\n",
    "            res[count]={\n",
    "                'name':name,\n",
    "                'img':img,\n",
    "                'link':link,\n",
    "                'description':description\n",
    "            }\n",
    "            count+=1\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubanMovie(Douban):\n",
    "    \n",
    "    \n",
    "    def get_nowshowing_movies(self):\n",
    "        response = self.request_get('https://movie.douban.com/cinema/nowplaying')\n",
    "        soup = BeautifulSoup(response.text)\n",
    "        nowshowing_movies = soup.find(class_='lists').find_all(class_='list-item')\n",
    "        movies_list = []\n",
    "        for nowshowing_movie in nowshowing_movies:\n",
    "            movie = {\n",
    "                'movie_id':nowshowing_movie.attrs['id'],\n",
    "                'title':nowshowing_movie.attrs['data-title'],\n",
    "                'actors':nowshowing_movie.attrs['data-actors'],\n",
    "                'director':nowshowing_movie.attrs['data-director'],\n",
    "                'score':nowshowing_movie.attrs['data-score'],\n",
    "                'release':nowshowing_movie.attrs['data-release'],\n",
    "                'region':nowshowing_movie.attrs['data-region']\n",
    "            }\n",
    "            movies_list.append(movie)\n",
    "            print(movie)\n",
    "        return movies_list\n",
    "    \n",
    "    def get_movie_info(self, url):\n",
    "        response = self.request_get(url)\n",
    "        soup = BeautifulSoup(response.text)\n",
    "        \n",
    "        movie = {\n",
    "            'movie_id':re.findall(r'https://movie.douban.com/subject/([0-9]*)',url)[0],\n",
    "            'title':soup.find(property=\"v:itemreviewed\").text.split()[0],\n",
    "            'score':soup.find(property=\"v:average\").text,\n",
    "            'release':re.findall(r'[(](.*?)[)]', soup.find(class_=\"year\").text)[0]\n",
    "        }\n",
    "        for info in soup.find(id='info').text.split('\\n'):\n",
    "            if '导演' in info:\n",
    "                movie['director'] = info.split(': ')[1]\n",
    "            elif '主演' in info:\n",
    "                movie['actors'] = info.split(': ')[1]\n",
    "            elif '国家' in info:\n",
    "                movie['region'] = info.split(': ')[1]\n",
    "            elif '主演' in info:\n",
    "                movie['actors'] = info.split(': ')[1]\n",
    "        print(movie)\n",
    "        return movie\n",
    "\n",
    "    def get_top_250(self):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def get_recently_hot_movie(self, **kwargs):\n",
    "        pass\n",
    "    \n",
    "    @run_time\n",
    "    def get_comments(self, **kwargs):\n",
    "        \n",
    "        count = 0\n",
    "        queue = Queue()\n",
    "        producer_running = True\n",
    "        \n",
    "        def producer(url=None, movie_id=None, page_max=50, page=0):\n",
    "            nonlocal producer_running\n",
    "            if producer_running is True and page <= page_max:\n",
    "                movie_url = None\n",
    "                if movie_id is not None:\n",
    "                    movie_url = 'https://movie.douban.com/subject/{}/comments'.format(str(movie_id))\n",
    "                else:\n",
    "                    movie_url = url\n",
    "                print(\"开始爬取第{0}页***********************************************************************：\".format(page+1))\n",
    "                params={\n",
    "                    'start':page*20,\n",
    "                    'limit':20,\n",
    "                    'status':'P',\n",
    "                    'sort':'new_score'\n",
    "                }\n",
    "                html = self.session_get(movie_url, params=params)\n",
    "                soup = BeautifulSoup(html.content, 'html.parser')\n",
    "                comments = soup.find(id='comments').find_all(class_='comment-item')\n",
    "                if len(comments) > 1:\n",
    "                    queue.put(comments)\n",
    "                    producer(url=movie_url, page=page+1, page_max=page_max)\n",
    "                else:\n",
    "                    producer_running=False\n",
    "            else:\n",
    "                producer_running=False\n",
    "            \n",
    "        def customer():\n",
    "            nonlocal count\n",
    "            while producer_running is True or queue.empty() is False:\n",
    "                comments = queue.get()\n",
    "                for comment in comments:\n",
    "                    content = comment.find(class_='comment-content').text\n",
    "                    user_name = comment.find(class_='comment-info').a.text\n",
    "                    comment_time = comment.find(class_='comment-info').find(class_='comment-time').attrs['title']\n",
    "                    \n",
    "                    print('comment:{}, user_name:{}, comment_time:{}'.format(content,user_name,comment_time))\n",
    "                    count += 1\n",
    "                print(\"count={}\".format(count))\n",
    "                time.sleep(int(random.choice([0.5, 0.2, 0.3])))\n",
    "                \n",
    "        threads = list()\n",
    "        \n",
    "        thread_p = threading.Thread(target=producer, kwargs=kwargs)\n",
    "        thread_p.start()\n",
    "        threads.append(thread_p)\n",
    "        \n",
    "        for _ in range(10):\n",
    "            thread_c = threading.Thread(target=customer)\n",
    "            thread_c.start()\n",
    "            threads.append(thread_c)\n",
    "            \n",
    "        for thread in threads:\n",
    "            thread.join(timeout=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubanMovie(Douban):\n",
    "    \n",
    "    @run_time\n",
    "    def get_comment(self, url=None, movie_id=None, page_max=50):\n",
    "        movie_url = None\n",
    "#         if movie_name is not None:\n",
    "            \n",
    "        if movie_id is not None:\n",
    "            movie_url = 'https://movie.douban.com/subject/{}/comments'.format(str(movie_id))\n",
    "        else:\n",
    "            movie_url = url\n",
    "        count = 0\n",
    "        for page in range(page_max+1):\n",
    "            print(\"开始爬取第{0}页***********************************************************************：\".format(page+1))\n",
    "            params={\n",
    "                'start':page*20,\n",
    "                'limit':20,\n",
    "                'status':'P',\n",
    "                'sort':'new_score'\n",
    "            }\n",
    "            html = self.session_get(movie_url, params=params)\n",
    "            print(html.url)\n",
    "            soup = BeautifulSoup(html.content, 'html.parser')\n",
    "            comments = soup.find(id='comments').find_all(class_='comment-item')\n",
    "            if len(comments) > 1:\n",
    "                for comment in comments:\n",
    "                    content = comment.find(class_='comment-content').text\n",
    "                    user_name = comment.find(class_='comment-info').a.text\n",
    "                    comment_time = comment.find(class_='comment-info').find(class_='comment-time').attrs['title']\n",
    "                    print('comment:{}, user_name:{}, comment_time:{}'.format(content,user_name,comment_time))\n",
    "                    count += 1\n",
    "                print(\"count={}\".format(count))\n",
    "                time.sleep(int(random.choice([0.5, 0.2, 0.3])))\n",
    "            else:\n",
    "                print(\"大约共{0}页评论\".format(page+1))\n",
    "                break\n",
    "            \n",
    "#     def get_review_by_url(self, url , page_max=50):\n",
    "#         res = dict()\n",
    "#         self.set_url(url)\n",
    "#         count = 0\n",
    "#         for page in range(page_max+1):\n",
    "#             print(\"开始爬取第{0}页***********************************************************************：\".format(page+1))\n",
    "#             self.set_params({\n",
    "#                 'start':page*20\n",
    "#             })\n",
    "#             html = self.session_get()\n",
    "#             soup = BeautifulSoup(html.content, 'html.parser')\n",
    "#             reviews = soup.find(class_='review-list').find_all(class_='review-item')\n",
    "#             for review in reviews:\n",
    "#                 user = review.find(class_='main-hd')\n",
    "#                 user_name = user.find(class_='name').text\n",
    "#                 user_link = user.find(class_='name').a.get(href)\n",
    "                \n",
    "#                 content = review.find(class_='main-bd')\n",
    "#                 title = content.find('h2').text\n",
    "#                 link = content.find('h2').a.get('href')\n",
    "#                 self.set_url(link)\n",
    "#                 review_response = self.request_get()\n",
    "#                 review_soup = BeautifulSoup(review_response, 'html.parser')\n",
    "#                 article = review_soup.find(class_='article')\n",
    "#                 review_content = article.find(class_='review-content').text\n",
    "#                 res[title] = {\n",
    "#                     'user_name':user_name,\n",
    "#                     'user_link':user_link,\n",
    "#                     'link':link,\n",
    "#                     'review_content':review_content\n",
    "#                 }\n",
    "#             return res\n",
    "        \n",
    "#     def get_review_by_id(self, movie_id, page_max=50):\n",
    "#         url = 'https://movie.douban.com/subject/{}/reviews'.format(str(movie_id))\n",
    "#         self.get_review_by_url(url, page_max)\n",
    "    \n",
    "    def search(self, movie_name):\n",
    "        result = super().search(movie_name, cat='1002')\n",
    "        return result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
